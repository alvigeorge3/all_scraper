# üöÄ Blinkit Scraper Demo Guide

This guide explains how to run the full scraper demo, verify the data, and visualize it on the dashboard.

## 1. Setup

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Configure Supabase
1. Create a project at [supabase.com](https://supabase.com).
2. Go to **SQL Editor** and run this query to create the table:
   ```sql
   create table blinkit_products (
     id bigint generated by default as identity primary key,
     product_id text,
     name text,
     price numeric,
     mrp numeric,
     availability text,
     inventory integer,
     category text,
     subcategory text,
     brand text,
     store_id text,
     scraped_at timestamp with time zone default timezone('utc'::text, now()),
     pincode_input text,
     url text,
     weight text,
     eta text,
     group_id text,
     merchant_type text,
     clicked_label text,
     base_product_id text,
     shelf_life_in_hours integer,
     base_product_id text,
     shelf_life_in_hours integer
     -- Removed unique constraint to allow historical data
   );
   ```
   > **Note**: If you already created the table, run this logic to add missing columns:
   ```sql
   ALTER TABLE blinkit_products 
   ADD COLUMN IF NOT EXISTS weight text,
   ADD COLUMN IF NOT EXISTS eta text,
   ADD COLUMN IF NOT EXISTS group_id text,
   ADD COLUMN IF NOT EXISTS merchant_type text,
   ADD COLUMN IF NOT EXISTS clicked_label text,
   ADD COLUMN IF NOT EXISTS base_product_id text,
   ADD COLUMN IF NOT EXISTS shelf_life_in_hours integer;
   ```
3. Get your `SUPABASE_URL` and `SUPABASE_KEY` (API Settings).
4. Create a `.env` file in `c:/scrapers/scraper_blinkit/.env`:
   ```env
   SUPABASE_URL=your_url_here
   SUPABASE_KEY=your_key_here
   ```

### ‚ö†Ô∏è Enabling Historical Data (For Existing Tables)
If you created the table earlier, you must remove the unique constraint to allow saving multiple versions of the same product (history). Run this in SQL Editor:
```sql
ALTER TABLE blinkit_products DROP CONSTRAINT IF EXISTS blinkit_products_product_id_pincode_input_key;
```

## 2. Running the Scraper

### Option A: Assortment Run
Scrapes a category or homepage for a specific pincode.
```bash
python run_blinkit_assortment.py --pincode 560001 --url "https://blinkit.com/cn/vegetables-fruits/cid/1487"
```
*Output*: `blinkit_assortment_YYYYMMDD_HHMMSS.csv`

### Option B: Availability Run
Scrapes specific URLs from `blinkit_input.xlsx`.
```bash
python run_blinkit_availability.py
```
*Output*: `blinkit_availability_YYYYMMDD_HHMMSS.csv`

### Option C: High Performance Run (Parallel)
Run 4 browsers simultaneously to process large lists (e.g. 100 pincodes).
```bash
python run_blinkit_parallel.py
```
*   **Speed**: ~4x faster than standard mode.
*   **Input**: Reads `pin_codes_100.xlsx`.
*   **Output**: `blinkit_availability_parallel_....csv`.

## 3. Uploading Data to Dashboard
Once you have a CSV file, upload it to the Supabase cloud:
```bash
python upload_blinkit_data.py blinkit_assortment_20260118_123000.csv
```

## 4. Viewing the Dashboard
Launch the visual interface:
```bash
streamlit run dashboard/app_blinkit.py
```
*   Opens in your browser at `http://localhost:8501`.
*   Explore Metrics, Charts, and Search products.
